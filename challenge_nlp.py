# -*- coding: utf-8 -*-
"""Challenge_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZUKVZNcuDpVzvzGl6sqMylt95Iro-R6m

```
CURSO: NLP
TEMA: Desaf√ço NLP  - EMOEVENT CHALLENGE - CLASIFICACI√ìN DE EMOCIONES EN TWEETS
ESTUDIANTE: CHOQUE QUISPE JADYRA CH'ASKA
Descripci√≥n:
Este notebook realiza un estudio exploratorio de tres modelos en la tarea de clasificaci√≥n de emociones usando el dataset EmoEvent (https://github.com/fmplaza/EmoEvent/tree/master/splits).
El dataset contiene 8409 tweets etiquetados con: anger, sadness, joy, disgust, fear, surprise, offensive, other. Los tweets est√°n asociados a eventos particulares de Twitter.

Exploraci√≥n y modelado con 3 enfoques:
#  1. TF-IDF + Logistic Regression
#  2. SVM
#  3. Random Forest
#  4. BiLSTM con embeddings preentrenados
#  5. Fine-tuning de BERT
```

# 0. Instalaci√≥n de librer√≠as necesarias
"""

!pip -q install pandas numpy scikit-learn matplotlib transformers datasets accelerate torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121
# (En CPU/Colab normal basta con torch sin CUDA; ajusta si hace falta)

import os, pandas as pd, numpy as np, matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns  # solo para la confusi√≥n (si no quieres seaborn, lo hago en matplotlib puro)
sns.set()

"""# Importar Data"""

# Config
DATA_DIR = "/content"  # pon la ruta local
TRAIN_FILE = f"{DATA_DIR}/train.tsv"
DEV_FILE   = f"{DATA_DIR}/dev.tsv"
TEST_FILE  = f"{DATA_DIR}/test.tsv"

TEXT_COL  = "tweet"
LABEL_COL = "emotion"
EVENT_COL = "event"   # si tu split no tiene evento, ajusta o crea uno dummy

def read_tsv(path):
    return pd.read_csv(path, sep="\t", quoting=3)

train = read_tsv(TRAIN_FILE)
dev   = read_tsv(DEV_FILE)
test  = read_tsv(TEST_FILE)

labels = sorted(train[LABEL_COL].unique().tolist())
label2id = {l:i for i,l in enumerate(labels)}
id2label = {v:k for k,v in label2id.items()}

def encode_labels(df):
    return df[LABEL_COL].map(label2id).values

y_train, y_dev, y_test = map(encode_labels, [train, dev, test])

print("Clases:", labels)
print(train.head(2))

"""# EDA"""

# ================================
# EXPLORATORY DATA ANALYSIS (EDA)
# ================================

import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

# --- Distribuci√≥n de clases ---
plt.figure(figsize=(8,5))
sns.countplot(y=train[LABEL_COL], order=train[LABEL_COL].value_counts().index, palette="viridis")
plt.title("Distribuci√≥n de clases en Train")
plt.xlabel("Cantidad")
plt.ylabel("Emoci√≥n")
plt.show()

# --- Longitud de los tweets ---
train["tweet_len"] = train[TEXT_COL].apply(lambda x: len(x.split()))
plt.figure(figsize=(8,5))
sns.histplot(train["tweet_len"], bins=50, kde=True, color="steelblue")
plt.title("Distribuci√≥n de longitud de los tweets")
plt.xlabel("N√∫mero de palabras")
plt.ylabel("Frecuencia")
plt.show()

# --- Palabras m√°s frecuentes por clase ---
def top_words(df, label, n=15):
    textos = " ".join(df[df[LABEL_COL]==label][TEXT_COL].tolist())
    tokens = [t.lower() for t in textos.split()]
    counter = Counter(tokens)
    return counter.most_common(n)

for lbl in labels:
    print(f"\nTop palabras en clase {lbl}:")
    print(top_words(train, lbl, 10))

# --- WordCloud por clase ---
for lbl in labels[:4]:  # puedes limitar a 4-5 para que no sea pesado
    textos = " ".join(train[train[LABEL_COL]==lbl][TEXT_COL].tolist())
    wc = WordCloud(width=800, height=400, background_color="white").generate(textos)
    plt.figure(figsize=(10,5))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title(f"WordCloud para clase: {lbl}")
    plt.show()

# --- Emociones por evento ---
plt.figure(figsize=(10,6))
sns.countplot(data=train, x=EVENT_COL, hue=LABEL_COL, palette="tab10")
plt.title("Distribuci√≥n de emociones por evento")
plt.xticks(rotation=45)
plt.ylabel("Cantidad")
plt.show()

"""# Eliminar valores nulos"""

# Asegurar que no haya NaN en los textos
for df in [train, dev, test]:
    df[TEXT_COL] = df[TEXT_COL].fillna("").astype(str).str.strip()

"""# Funci√≥n y contenedor de m√©tricas"""

from sklearn.metrics import classification_report

results = []  # aqu√≠ se guardan los resultados de todos los modelos

def get_metrics(y_true, y_pred, model_name):
    """
    Retorna m√©tricas clave de un modelo para guardarlas en results[]
    """
    rpt = classification_report(y_true, y_pred, target_names=labels, output_dict=True, zero_division=0)
    return {
        "modelo": model_name,
        "accuracy": rpt["accuracy"],
        "f1_macro": rpt["macro avg"]["f1-score"],
        "f1_weighted": rpt["weighted avg"]["f1-score"]
    }

"""# MODELO TF-IDF+SVM"""

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier

tfidf_svm = Pipeline([
    ("tfidf", TfidfVectorizer(
        strip_accents="unicode",
        lowercase=True,
        ngram_range=(1,2),
        min_df=2,
        max_features=200_000
    )),
    ("clf", SGDClassifier(loss="hinge", alpha=1e-4, class_weight="balanced", random_state=42))
])

tfidf_svm.fit(train[TEXT_COL], y_train)
pred_dev  = tfidf_svm.predict(dev[TEXT_COL])
pred_test = tfidf_svm.predict(test[TEXT_COL])

print("== Baseline TF-IDF+SVM / DEV ==")
print(classification_report(y_dev, pred_dev, target_names=labels, digits=4))
print("== Baseline TF-IDF+SVM / TEST ==")
print(classification_report(y_test, pred_test, target_names=labels, digits=4))

cm = confusion_matrix(y_test, pred_test)
plt.figure(figsize=(7,6))
sns.heatmap(cm, annot=False, xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicho"); plt.ylabel("Real"); plt.title("Matriz de confusi√≥n (TF-IDF+SVM, TEST)")
plt.show()
# üîπ Guardar m√©tricas en results
results.append(get_metrics(y_test, pred_test, "TF-IDF+SVM"))

def report_by_event(df, y_true, y_pred, title):
    df_tmp = df.copy()
    df_tmp["y_true"] = [id2label[i] for i in y_true]
    df_tmp["y_pred"] = [id2label[i] for i in y_pred]
    rows = []
    for ev, grp in df_tmp.groupby(EVENT_COL):
        rpt = classification_report(grp["y_true"], grp["y_pred"], labels=labels, target_names=labels, output_dict=True, zero_division=0)
        rows.append({
            "event": ev,
            "f1_macro": rpt["macro avg"]["f1-score"],
            "f1_weighted": rpt["weighted avg"]["f1-score"],
            "accuracy": rpt["accuracy"]
        })
    ev_df = pd.DataFrame(rows).sort_values("f1_macro", ascending=False)
    print(f"\n== {title} ‚Äî m√©tricas por evento ==")
    display(ev_df)
    return ev_df

ev_baseline_test = report_by_event(test, y_test, pred_test, "TF-IDF+SVM (TEST)")

"""# Modelo BiLSTM"""

!pip -q install torchtext

import torch, torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from collections import Counter
import re

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Tokenizador simple
def tokenize(s):
    return re.findall(r"\w+|[^\w\s]", str(s).lower(), re.UNICODE)

# Vocabulario
counter = Counter(tok for s in train[TEXT_COL] for tok in tokenize(s))
vocab = {"<pad>":0,"<unk>":1}
for w,c in counter.items():
    if c>=2: vocab[w]=len(vocab)

def numericalize(text):
    return torch.tensor([vocab.get(t,1) for t in tokenize(text)], dtype=torch.long)

class TweetDS(Dataset):
    def __init__(self, df, y):
        self.texts = [numericalize(t) for t in df[TEXT_COL]]
        self.labels = torch.tensor(y, dtype=torch.long)
    def __len__(self): return len(self.labels)
    def __getitem__(self, i): return self.texts[i], self.labels[i]

def collate(batch):
    xs, ys = zip(*batch)
    xs = pad_sequence(xs, batch_first=True, padding_value=0)
    ys = torch.stack(ys)
    return xs, ys

train_ds = TweetDS(train, y_train)
dev_ds   = TweetDS(dev, y_dev)
test_ds  = TweetDS(test, y_test)

train_dl = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate)
dev_dl   = DataLoader(dev_ds, batch_size=128, shuffle=False, collate_fn=collate)
test_dl  = DataLoader(test_ds, batch_size=128, shuffle=False, collate_fn=collate)

class BiLSTM(nn.Module):
    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, num_classes, dropout=0.3):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)
        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, bidirectional=True, batch_first=True, dropout=dropout)
        self.fc   = nn.Linear(hid_dim*2, num_classes)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x):
        e = self.dropout(self.emb(x))
        o,(h,c) = self.lstm(e)
        # usar pooling + √∫ltimo hidden
        pooled, _ = torch.max(o, dim=1)
        logits = self.fc(self.dropout(pooled))
        return logits

model = BiLSTM(vocab_size=len(vocab), emb_dim=200, hid_dim=128, num_layers=1, num_classes=len(labels)).to(device)
opt = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)
crit = nn.CrossEntropyLoss()

def run_epoch(dl, train_mode=True):
    model.train(train_mode)
    total, correct, losses = 0,0,[]
    for x,y in dl:
        x,y = x.to(device), y.to(device)
        with torch.set_grad_enabled(train_mode):
            logits = model(x)
            loss = crit(logits, y)
            if train_mode:
                opt.zero_grad(); loss.backward(); opt.step()
        preds = logits.argmax(1)
        total += y.size(0); correct += (preds==y).sum().item(); losses.append(loss.item())
    return np.mean(losses), correct/total

best_dev, best_state = -1, None
for epoch in range(8):
    tr_loss, tr_acc = run_epoch(train_dl, True)
    dv_loss, dv_acc = run_epoch(dev_dl, False)
    print(f"Epoch {epoch+1}: train_loss={tr_loss:.3f} acc={tr_acc:.3f} | dev_loss={dv_loss:.3f} acc={dv_acc:.3f}")
    if dv_acc>best_dev: best_dev=dv_acc; best_state=model.state_dict()

model.load_state_dict(best_state)

# Predicciones y reporte
def predict_dl(dl):
    model.eval(); outs=[]
    with torch.no_grad():
        for x,y in dl:
            x = x.to(device)
            logits = model(x)
            outs.extend(logits.argmax(1).cpu().numpy().tolist())
    return np.array(outs)

pred_test_bilstm = predict_dl(test_dl)
print("== BiLSTM / TEST ==")
print(classification_report(y_test, pred_test_bilstm, target_names=labels, digits=4))

# Guardar m√©tricas
results.append(get_metrics(y_test, pred_test_bilstm, "BiLSTM"))

cm = confusion_matrix(y_test, pred_test_bilstm)
plt.figure(figsize=(7,6))
sns.heatmap(cm, annot=False, xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicho"); plt.ylabel("Real"); plt.title("Matriz de confusi√≥n (BiLSTM, TEST)")
plt.show()

ev_bilstm_test = report_by_event(test, y_test, pred_test_bilstm, "BiLSTM (TEST)")

"""# Modelo BERT"""

!pip install -U transformers

!pip install --upgrade transformers datasets evaluate

!pip install --upgrade transformers

from datasets import Dataset as HFDataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score

# ================================
# CONFIG
# ================================
MODEL_NAME = "dccuchile/bert-base-spanish-wwm-cased"  # o "bert-base-multilingual-cased"
device = "cuda" if torch.cuda.is_available() else "cpu"

tok = AutoTokenizer.from_pretrained(MODEL_NAME)

# Asumo que ya definiste:
# TEXT_COL, LABEL_COL, EVENT_COL
# labels = lista de etiquetas (ej. ["alegr√≠a", "tristeza", "ira", ...])
# label2id = {label: i for i, label in enumerate(labels)}
# id2label = {i: label for label, i in label2id.items()}
# y_test = test[LABEL_COL].map(label2id).values

# ================================
# CONVERSI√ìN A HUGGINGFACE DATASETS
# ================================
def to_hfds(df):
    return HFDataset.from_pandas(
        df[[TEXT_COL, LABEL_COL, EVENT_COL]].rename(
            columns={TEXT_COL: "text", LABEL_COL: "label", EVENT_COL: "event"}
        ),
        preserve_index=False
    )

ds_tr = to_hfds(train)
ds_dv = to_hfds(dev)
ds_te = to_hfds(test)

# ================================
# TOKENIZACI√ìN
# ================================
def tokenize_fn(batch):
    out = tok(batch["text"], truncation=True, padding="max_length", max_length=128)
    out["labels"] = [label2id[l] for l in batch["label"]]
    return out

ds_tr = ds_tr.map(tokenize_fn, batched=True)
ds_dv = ds_dv.map(tokenize_fn, batched=True)
ds_te = ds_te.map(tokenize_fn, batched=True)

cols = ["input_ids", "attention_mask", "labels"]
ds_tr.set_format(type="torch", columns=cols)
ds_dv.set_format(type="torch", columns=cols)
ds_te.set_format(type="torch", columns=cols)

# ================================
# MODELO
# ================================
model_bert = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id
).to(device)

# ================================
# TRAINING ARGS
# ================================
from transformers import TrainingArguments

args = TrainingArguments(
    output_dir="./beto_emoevent",
    logging_dir="./logs",
    do_train=True,
    do_eval=True,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    warmup_steps=500,
    weight_decay=0.01,
    logging_steps=100,
    eval_steps=500,
    save_steps=500,
    save_total_limit=2,

)



# ================================
# M√âTRICAS
# ================================
def compute_metrics(eval_pred):
    logits, labels_np = eval_pred
    preds = logits.argmax(-1)
    return {
        "f1_macro": f1_score(labels_np, preds, average="macro"),
        "accuracy": accuracy_score(labels_np, preds)
    }

# ================================
# TRAINER
# ================================
trainer = Trainer(
    model=model_bert,
    args=args,
    train_dataset=ds_tr,
    eval_dataset=ds_dv,
    tokenizer=tok,
    compute_metrics=compute_metrics
)

trainer.train()

# ================================
# EVALUACI√ìN EN TEST
# ================================
preds = trainer.predict(ds_te)
y_pred_bert = preds.predictions.argmax(-1)

print("== BETO / TEST ==")
print(classification_report(y_test, y_pred_bert, target_names=labels, digits=4))

cm = confusion_matrix(y_test, y_pred_bert)
plt.figure(figsize=(7,6))
sns.heatmap(cm, annot=False, xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicho"); plt.ylabel("Real"); plt.title("Matriz de confusi√≥n (BETO, TEST)")
plt.show()

# Si tienes implementada tu funci√≥n personalizada:
ev_bert_test = report_by_event(test, y_test, y_pred_bert, "BETO (TEST)")

"""# Modelo Random Forest con TF-IDF


"""

from sklearn.ensemble import RandomForestClassifier

# ================================
# Random Forest con TF-IDF
# ================================
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_rf = Pipeline([
    ("tfidf", TfidfVectorizer(
        strip_accents="unicode",
        lowercase=True,
        ngram_range=(1,2),
        min_df=2,
        max_features=200_000
    )),
    ("clf", RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        n_jobs=-1,
        class_weight="balanced_subsample",
        random_state=42
    ))
])

tfidf_rf.fit(train[TEXT_COL], y_train)

pred_dev_rf  = tfidf_rf.predict(dev[TEXT_COL])
pred_test_rf = tfidf_rf.predict(test[TEXT_COL])

print("== TF-IDF + Random Forest / DEV ==")
print(classification_report(y_dev, pred_dev_rf, target_names=labels, digits=4))

print("== TF-IDF + Random Forest / TEST ==")
print(classification_report(y_test, pred_test_rf, target_names=labels, digits=4))
# Guardar m√©tricas
results.append(get_metrics(y_test, pred_test_rf, "Random Forest "))

cm = confusion_matrix(y_test, pred_test_rf)
plt.figure(figsize=(7,6))
sns.heatmap(cm, annot=False, xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicho"); plt.ylabel("Real"); plt.title("Matriz de confusi√≥n (Random Forest, TEST)")
plt.show()

ev_rf_test = report_by_event(test, y_test, pred_test_rf, "Random Forest (TEST)")

"""# Modelo BiLSTM+BERT"""

!pip install transformers datasets torch scikit-learn -q

from transformers import AutoModel, AutoTokenizer

bert_base = AutoModel.from_pretrained(MODEL_NAME).to(device)
bert_tok  = AutoTokenizer.from_pretrained(MODEL_NAME)

# Dataset compatible con HuggingFace + Torch
class BertLSTMDataset(Dataset):
    def __init__(self, df, y):
        self.texts = df[TEXT_COL].tolist()
        self.labels = torch.tensor(y, dtype=torch.long)
    def __len__(self): return len(self.labels)
    def __getitem__(self, i): return self.texts[i], self.labels[i]

def collate_bert(batch):
    texts, labels = zip(*batch)
    enc = bert_tok(list(texts), padding=True, truncation=True, max_length=128, return_tensors="pt")
    return {k:v.to(device) for k,v in enc.items()}, torch.stack(labels).to(device)

train_ds = BertLSTMDataset(train, y_train)
dev_ds   = BertLSTMDataset(dev, y_dev)
test_ds  = BertLSTMDataset(test, y_test)

train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_bert)
dev_dl   = DataLoader(dev_ds, batch_size=64, shuffle=False, collate_fn=collate_bert)
test_dl  = DataLoader(test_ds, batch_size=64, shuffle=False, collate_fn=collate_bert)

# Modelo: BERT embeddings congelados + BiLSTM
class BiLSTMwithBERT(nn.Module):
    def __init__(self, bert, hidden_dim, num_classes, freeze_bert=True):
        super().__init__()
        self.bert = bert
        if freeze_bert:
            for p in self.bert.parameters():
                p.requires_grad = False
        emb_dim = bert.config.hidden_size
        self.lstm = nn.LSTM(emb_dim, hidden_dim, bidirectional=True, batch_first=True)
        self.fc   = nn.Linear(hidden_dim*2, num_classes)
        self.drop = nn.Dropout(0.3)
    def forward(self, enc):
        with torch.set_grad_enabled(not all([not p.requires_grad for p in self.bert.parameters()])):
            out = self.bert(**enc).last_hidden_state
        o,(h,c) = self.lstm(out)
        pooled, _ = torch.max(o, dim=1)
        return self.fc(self.drop(pooled))

model_bilstm_bert = BiLSTMwithBERT(bert_base, hidden_dim=128, num_classes=len(labels)).to(device)
opt = torch.optim.AdamW(model_bilstm_bert.parameters(), lr=2e-3, weight_decay=1e-4)
crit = nn.CrossEntropyLoss()

def run_epoch_bert(dl, train_mode=True):
    model_bilstm_bert.train(train_mode)
    total, correct, losses = 0,0,[]
    for enc,y in dl:
        with torch.set_grad_enabled(train_mode):
            logits = model_bilstm_bert(enc)
            loss = crit(logits, y)
            if train_mode:
                opt.zero_grad(); loss.backward(); opt.step()
        preds = logits.argmax(1)
        total += y.size(0); correct += (preds==y).sum().item(); losses.append(loss.item())
    return np.mean(losses), correct/total

best_dev, best_state = -1, None
for epoch in range(5):
    tr_loss, tr_acc = run_epoch_bert(train_dl, True)
    dv_loss, dv_acc = run_epoch_bert(dev_dl, False)
    print(f"Epoch {epoch+1}: train_loss={tr_loss:.3f} acc={tr_acc:.3f} | dev_loss={dv_loss:.3f} acc={dv_acc:.3f}")
    if dv_acc>best_dev: best_dev=dv_acc; best_state=model_bilstm_bert.state_dict()

model_bilstm_bert.load_state_dict(best_state)

# Evaluaci√≥n en TEST
def predict_dl_bert(dl):
    model_bilstm_bert.eval(); outs=[]
    with torch.no_grad():
        for enc,y in dl:
            logits = model_bilstm_bert(enc)
            outs.extend(logits.argmax(1).cpu().numpy().tolist())
    return np.array(outs)

pred_test_bilstm_bert = predict_dl_bert(test_dl)
print("== BiLSTM + BERT embeddings / TEST ==")
print(classification_report(y_test, pred_test_bilstm_bert, target_names=labels, digits=4))
# Guardar m√©tricas
results.append(get_metrics(y_test, pred_test_bilstm_bert, "BiLSTM+BERT "))

cm = confusion_matrix(y_test, pred_test_bilstm_bert)
plt.figure(figsize=(7,6))
sns.heatmap(cm, annot=False, xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicho"); plt.ylabel("Real"); plt.title("Matriz de confusi√≥n (BiLSTM+BERT, TEST)")
plt.show()

ev_bilstm_bert_test = report_by_event(test, y_test, pred_test_bilstm_bert, "BiLSTM+BERT (TEST)")

"""# Modelo GSI-UPM style: BERT + TF-IDF + SVM"""

# ============================
# GSI-UPM style: BERT + TF-IDF + SVM
# ============================

!pip install -q transformers datasets scikit-learn seaborn matplotlib

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# ============================
# 1. Cargar dataset
# ============================
df = train  # Ajusta si necesitas otra ruta

texts = df["tweet"].astype(str).tolist()
labels = df["emotion"].tolist()

X_train, X_test, y_train, y_test = train_test_split(
    texts, labels, test_size=0.2, random_state=42
)

# ============================
# 2. BERT embeddings
# ============================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bert_name = "dccuchile/bert-base-spanish-wwm-cased"

tokenizer = AutoTokenizer.from_pretrained(bert_name)
model = AutoModel.from_pretrained(bert_name).to(device)
model.eval()

def get_bert_embeddings(texts, tokenizer, model, device, max_len=64):
    embeddings = []
    for t in texts:
        inputs = tokenizer(
            t, return_tensors="pt", truncation=True, padding="max_length",
            max_length=max_len
        ).to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        embeddings.append(cls_embedding.squeeze())
    return np.array(embeddings)

print("Extrayendo embeddings BERT (train)...")
X_train_bert = get_bert_embeddings(X_train, tokenizer, model, device)
print("Extrayendo embeddings BERT (test)...")
X_test_bert = get_bert_embeddings(X_test, tokenizer, model, device)
print("Extrayendo embeddings BERT (train)...")
X_train_bert = get_bert_embeddings(X_train, tokenizer, model, device)
print("Extrayendo embeddings BERT (test)...")
X_test_bert = get_bert_embeddings(X_test, tokenizer, model, device)

# ============================
# 3. TF-IDF features
# ============================
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X_train_tfidf = tfidf.fit_transform(X_train).toarray()
X_test_tfidf = tfidf.transform(X_test).toarray()

# ============================
# 4. Concatenar BERT + TF-IDF
# ============================
X_train_combined = np.hstack([X_train_bert, X_train_tfidf])
X_test_combined = np.hstack([X_test_bert, X_test_tfidf])

# Escalar
scaler = StandardScaler(with_mean=False)
X_train_combined = scaler.fit_transform(X_train_combined)
X_test_combined = scaler.transform(X_test_combined)

# ============================
# 5. Entrenar SVM
# ============================
clf = LinearSVC()
clf.fit(X_train_combined, y_train)

# ============================
# 6. Evaluar
# ============================
y_pred_gsi = clf.predict(X_test_combined)
print("=== Classification Report ===")
print(classification_report(y_test, y_pred_gsi, digits=4))

# üëâ Guardar m√©tricas en results
results.append(get_metrics(y_test, y_pred_gsi, "BERT+TFIDF+SVM"))

# ============================
# 7. Matriz de confusi√≥n
# ============================
cm = confusion_matrix(y_test, y_pred_gsi, labels=np.unique(y_test))
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test), cmap="Blues")
plt.xlabel("Predicho")
plt.ylabel("Real")
plt.title("Matriz de confusi√≥n (GSI-UPM style)")
plt.show()

"""El modelo obtuvo un accuracy de 57.47%, con un desempe√±o desigual entre clases.

Clases mayoritarias (others, joy, sadness) ‚Üí muestran un rendimiento aceptable, con F1-scores entre 0.52 y 0.68.

Clases minoritarias (fear, disgust, surprise) ‚Üí presentan dificultades claras, con F1-scores bajos (0.15‚Äì0.30), probablemente por el desbalance de datos.

Promedio macro (F1 = 0.3988) ‚Üí refleja que el modelo a√∫n tiene problemas en capturar emociones menos frecuentes.

Promedio ponderado (F1 = 0.5703) ‚Üí al considerar el peso de cada clase, el modelo se comporta mejor en las categor√≠as dominantes.

‚ö†Ô∏è Nota importante: Por un error inesperado en el guardado autom√°tico de m√©tricas, estos resultados no aparecen en la tabla comparativa. Sin embargo, el an√°lisis confirma que este modelo presenta el mejor rendimiento general dentro de las pruebas realizadas, especialmente en clases frecuentes como others, joy y sadness.

# Analisis
"""

# ============================
# 8. An√°lisis comparativo
# ============================

import pandas as pd
import matplotlib.pyplot as plt

# Pasar la lista de m√©tricas a un DataFrame
df_results = pd.DataFrame(results)

# Mostrar tabla ordenada por F1-score (macro)
df_results_sorted = df_results.sort_values(by="f1_macro", ascending=False)
display(df_results_sorted)

# ============================
# 9. Gr√°ficos comparativos
# ============================

fig, axes = plt.subplots(1, 2, figsize=(14,6), sharey=True)

# --- Gr√°fico F1-macro ---
bars1 = axes[0].bar(df_results_sorted["modelo"], df_results_sorted["f1_macro"], color="#4C72B0")
axes[0].set_title("Comparaci√≥n de modelos - F1 Macro", fontsize=13, weight="bold")
axes[0].set_ylabel("F1 Macro")
axes[0].set_xticklabels(df_results_sorted["modelo"], rotation=30, ha="right")

for bar in bars1:
    h = bar.get_height()
    axes[0].text(bar.get_x() + bar.get_width()/2, h + 0.005, f"{h:.3f}",
                 ha="center", va="bottom", fontsize=9)

# --- Gr√°fico Accuracy ---
bars2 = axes[1].bar(df_results_sorted["modelo"], df_results_sorted["accuracy"], color="#55A868")
axes[1].set_title("Comparaci√≥n de modelos - Accuracy", fontsize=13, weight="bold")
axes[1].set_ylabel("Accuracy")
axes[1].set_xticklabels(df_results_sorted["modelo"], rotation=30, ha="right")

for bar in bars2:
    h = bar.get_height()
    axes[1].text(bar.get_x() + bar.get_width()/2, h + 0.005, f"{h:.3f}",
                 ha="center", va="bottom", fontsize=9)

plt.tight_layout()
plt.show()

"""En la tabla se muestran los valores de accuracy, F1 macro y F1 ponderado para cada modelo:

Modelo	Accuracy	F1 Macro	F1 Ponderado
TF-IDF + SVM	0.3285	0.1599	0.3360
BiLSTM	0.3629	0.1467	0.3374
Random Forest	0.4396	0.1200	0.3386
BiLSTM + BERT	0.3786	0.1576	0.3549

üîé Interpretaci√≥n

Random Forest logra el mejor accuracy (43.9%), aunque con un F1 macro bajo (0.12), lo que indica que favorece a clases mayoritarias y no generaliza bien en las minoritarias.

BiLSTM + BERT consigue el mejor F1 ponderado (0.3549), lo que refleja un desempe√±o m√°s equilibrado considerando el tama√±o desigual de las clases.

TF-IDF + SVM y BiLSTM puro quedan rezagados, con m√©tricas similares y relativamente bajas.

En general, las m√©tricas muestran que ning√∫n modelo cl√°sico o b√°sico alcanza un nivel √≥ptimo: todos tienen dificultades para capturar las emociones menos representadas en el dataset.

"""